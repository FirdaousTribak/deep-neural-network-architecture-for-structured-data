{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "04aad7e3-3e40-43d7-8ba0-97749cfad8a4",
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras import layers\n",
    "import tensorflow_addons as tfa\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "66199794-9b30-4683-86c8-f87f2698daa1",
   "metadata": {},
   "source": [
    "# Prepare the data\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "f9583001-f932-449a-b531-782c9a4f572f",
   "metadata": {},
   "outputs": [],
   "source": [
    "#our features\n",
    "CSV_HEADER = [\n",
    "    \"age\",\n",
    "    \"workclass\",\n",
    "    \"fnlwgt\",\n",
    "    \"education\",\n",
    "    \"education_num\",\n",
    "    \"marital_status\",\n",
    "    \"occupation\",\n",
    "    \"relationship\",\n",
    "    \"race\",\n",
    "    \"gender\",\n",
    "    \"capital_gain\",\n",
    "    \"capital_loss\",\n",
    "    \"hours_per_week\",\n",
    "    \"native_country\",\n",
    "    \"income_bracket\",\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "eb56d5fc-76a7-45f9-b3cd-57fc12b001f5",
   "metadata": {},
   "outputs": [],
   "source": [
    "#import the train data\n",
    "train_data_url = (\n",
    "    \"https://archive.ics.uci.edu/ml/machine-learning-databases/adult/adult.data\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "ddd1372f-b21e-46cf-8341-0a3eaa4dd4bc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train dataset shape: (32561, 15)\n",
      "Test dataset shape: (16282, 15)\n"
     ]
    }
   ],
   "source": [
    "train_data = pd.read_csv(train_data_url, header=None, names=CSV_HEADER)\n",
    "#import test data\n",
    "test_data_url = (\"https://archive.ics.uci.edu/ml/machine-learning-databases/adult/adult.test\")\n",
    "test_data = pd.read_csv(test_data_url, header=None, names=CSV_HEADER)\n",
    "#display the shape of this file\n",
    "print(f\"Train dataset shape: {train_data.shape}\")\n",
    "print(f\"Test dataset shape: {test_data.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "6da8d411-e768-4c92-bec1-5b9204a2d42f",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Remove the first record (because it is not a valid data example) and a trailing 'dot' in the class labels\n",
    "test_data = test_data[1:]\n",
    "test_data.income_bracket = test_data.income_bracket.apply(\n",
    "    lambda value: value.replace(\".\", \"\")\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "3626cc60-5e39-40dc-a30e-5e5a223c2e80",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Now we store the training and test data in separate CSV files\n",
    "train_data_file = \"train_data.csv\"\n",
    "test_data_file = \"test_data.csv\"\n",
    "\n",
    "train_data.to_csv(train_data_file, index=False, header=False)\n",
    "test_data.to_csv(test_data_file, index=False, header=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a13d1a31-a01c-469c-a6a2-7ab1117128e1",
   "metadata": {},
   "source": [
    "# Define dataset metadata\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "4cd28e84-505b-4f8c-90d3-573d854f00ce",
   "metadata": {},
   "outputs": [],
   "source": [
    "# A list of the numerical feature names.\n",
    "NUMERIC_FEATURE_NAMES = [\n",
    "    \"age\",\n",
    "    \"education_num\",\n",
    "    \"capital_gain\",\n",
    "    \"capital_loss\",\n",
    "    \"hours_per_week\",\n",
    "]\n",
    "# A dictionary of the categorical features and their vocabulary.\n",
    "CATEGORICAL_FEATURES_WITH_VOCABULARY = {\n",
    "    \"workclass\": sorted(list(train_data[\"workclass\"].unique())),\n",
    "    \"education\": sorted(list(train_data[\"education\"].unique())),\n",
    "    \"marital_status\": sorted(list(train_data[\"marital_status\"].unique())),\n",
    "    \"occupation\": sorted(list(train_data[\"occupation\"].unique())),\n",
    "    \"relationship\": sorted(list(train_data[\"relationship\"].unique())),\n",
    "    \"race\": sorted(list(train_data[\"race\"].unique())),\n",
    "    \"gender\": sorted(list(train_data[\"gender\"].unique())),\n",
    "    \"native_country\": sorted(list(train_data[\"native_country\"].unique())),\n",
    "}\n",
    "# Name of the column to be used as instances weight.\n",
    "WEIGHT_COLUMN_NAME = \"fnlwgt\"\n",
    "# A list of the categorical feature names.\n",
    "CATEGORICAL_FEATURE_NAMES = list(CATEGORICAL_FEATURES_WITH_VOCABULARY.keys())\n",
    "# A list of all the input features.\n",
    "FEATURE_NAMES = NUMERIC_FEATURE_NAMES + CATEGORICAL_FEATURE_NAMES\n",
    "# A list of column default values for each feature.\n",
    "COLUMN_DEFAULTS = [\n",
    "    [0.0] if feature_name in NUMERIC_FEATURE_NAMES + [WEIGHT_COLUMN_NAME] else [\"NA\"]\n",
    "    for feature_name in CSV_HEADER\n",
    "]\n",
    "# The name of the target feature.\n",
    "TARGET_FEATURE_NAME = \"income_bracket\"\n",
    "# A list of the labels of the target features.\n",
    "TARGET_LABELS = [\" <=50K\", \" >50K\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "84a18bda-c6b4-4ac1-9767-cd9c1ba74bc2",
   "metadata": {},
   "source": [
    "# Configure the hyperparameters\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "28818ed6-acf6-481a-a794-ea1b3dcdcfcf",
   "metadata": {},
   "outputs": [],
   "source": [
    "LEARNING_RATE = 0.001\n",
    "WEIGHT_DECAY = 0.0001\n",
    "DROPOUT_RATE = 0.2\n",
    "BATCH_SIZE = 265\n",
    "NUM_EPOCHS = 15\n",
    "\n",
    "NUM_TRANSFORMER_BLOCKS = 3  # Number of transformer blocks.\n",
    "NUM_HEADS = 4  # Number of attention heads.\n",
    "EMBEDDING_DIMS = 16  # Embedding dimensions of the categorical features.\n",
    "MLP_HIDDEN_UNITS_FACTORS = [\n",
    "    2,\n",
    "    1,\n",
    "]  # MLP hidden layer units, as factors of the number of inputs.\n",
    "NUM_MLP_BLOCKS = 2  # Number of MLP blocks in the baseline model."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3977aefe-73f6-4e48-97c9-c1d5863a5ae8",
   "metadata": {},
   "source": [
    "# Implement data reading pipeline\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "4162f25d-a6a6-4130-880c-a11ead087923",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Acer\\anaconda3\\lib\\site-packages\\numpy\\core\\numeric.py:2453: FutureWarning: elementwise comparison failed; returning scalar instead, but in the future will perform elementwise comparison\n",
      "  return bool(asarray(a1 == a2).all())\n"
     ]
    }
   ],
   "source": [
    "#define an input function that reads and parses the file, then converts features and labels into atf.data.Dataset for training or evaluation\n",
    "target_label_lookup = layers.StringLookup(\n",
    "    vocabulary=TARGET_LABELS, mask_token=None, num_oov_indices=0\n",
    ")\n",
    "\n",
    "\n",
    "def prepare_example(features, target):\n",
    "    target_index = target_label_lookup(target)\n",
    "    weights = features.pop(WEIGHT_COLUMN_NAME)\n",
    "    return features, target_index, weights\n",
    "\n",
    "\n",
    "def get_dataset_from_csv(csv_file_path, batch_size=128, shuffle=False):\n",
    "    dataset = tf.data.experimental.make_csv_dataset(\n",
    "        csv_file_path,\n",
    "        batch_size=batch_size,\n",
    "        column_names=CSV_HEADER,\n",
    "        column_defaults=COLUMN_DEFAULTS,\n",
    "        label_name=TARGET_FEATURE_NAME,\n",
    "        num_epochs=1,\n",
    "        header=False,\n",
    "        na_value=\"?\",\n",
    "        shuffle=shuffle,\n",
    "    ).map(prepare_example, num_parallel_calls=tf.data.AUTOTUNE, deterministic=False)\n",
    "    return dataset.cache()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2f2e698e-880c-4441-94cb-f52123fb769e",
   "metadata": {},
   "source": [
    "# Implement a training and evaluation procedure\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "be43f07c-fd83-409a-b36b-5eb6a8a9333f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_experiment(\n",
    "    model,\n",
    "    train_data_file,\n",
    "    test_data_file,\n",
    "    num_epochs,\n",
    "    learning_rate,\n",
    "    weight_decay,\n",
    "    batch_size,\n",
    "):\n",
    "\n",
    "    optimizer = tfa.optimizers.AdamW(\n",
    "        learning_rate=learning_rate, weight_decay=weight_decay\n",
    "    )\n",
    "\n",
    "    model.compile(\n",
    "        optimizer=optimizer,\n",
    "        loss=keras.losses.BinaryCrossentropy(),\n",
    "        metrics=[keras.metrics.BinaryAccuracy(name=\"accuracy\")],\n",
    "    )\n",
    "\n",
    "    train_dataset = get_dataset_from_csv(train_data_file, batch_size, shuffle=True)\n",
    "    validation_dataset = get_dataset_from_csv(test_data_file, batch_size)\n",
    "\n",
    "    print(\"Start training the model...\")\n",
    "    history = model.fit(\n",
    "        train_dataset, epochs=num_epochs, validation_data=validation_dataset\n",
    "    )\n",
    "    print(\"Model training finished\")\n",
    "\n",
    "    _, accuracy = model.evaluate(validation_dataset, verbose=0)\n",
    "\n",
    "    print(f\"Validation accuracy: {round(accuracy * 100, 2)}%\")\n",
    "\n",
    "    return history"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "846ce8ba-d01e-406f-add0-b81a91350b79",
   "metadata": {},
   "source": [
    "# Create model inputs\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "b7d3307c-a706-4516-afb8-2730474b92ad",
   "metadata": {},
   "outputs": [],
   "source": [
    "#define the inputs for the models as a dictionary, where the key is the feature name, and the value is a keras.layers.Input tensor with the corresponding feature shape and data type\n",
    "def create_model_inputs():\n",
    "    inputs = {}\n",
    "    for feature_name in FEATURE_NAMES:\n",
    "        if feature_name in NUMERIC_FEATURE_NAMES:\n",
    "            inputs[feature_name] = layers.Input(\n",
    "                name=feature_name, shape=(), dtype=tf.float32\n",
    "            )\n",
    "        else:\n",
    "            inputs[feature_name] = layers.Input(\n",
    "                name=feature_name, shape=(), dtype=tf.string\n",
    "            )\n",
    "    return inputs"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "076ec73c-fa5e-4136-a73e-621e9bc78f90",
   "metadata": {},
   "source": [
    "# Encode features\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "19ec2e35-885a-414e-b3bd-5f2052146dd2",
   "metadata": {},
   "outputs": [],
   "source": [
    "def encode_inputs(inputs, embedding_dims):\n",
    "\n",
    "    encoded_categorical_feature_list = []\n",
    "    numerical_feature_list = []\n",
    "\n",
    "    for feature_name in inputs:\n",
    "        if feature_name in CATEGORICAL_FEATURE_NAMES:\n",
    "\n",
    "            # Get the vocabulary of the categorical feature.\n",
    "            vocabulary = CATEGORICAL_FEATURES_WITH_VOCABULARY[feature_name]\n",
    "\n",
    "            # Create a lookup to convert string values to an integer indices.\n",
    "            # Since we are not using a mask token nor expecting any out of vocabulary\n",
    "            # (oov) token, we set mask_token to None and  num_oov_indices to 0.\n",
    "            lookup = layers.StringLookup(\n",
    "                vocabulary=vocabulary,\n",
    "                mask_token=None,\n",
    "                num_oov_indices=0,\n",
    "                output_mode=\"int\",\n",
    "            )\n",
    "\n",
    "            # Convert the string input values into integer indices.\n",
    "            encoded_feature = lookup(inputs[feature_name])\n",
    "\n",
    "            # Create an embedding layer with the specified dimensions.\n",
    "            embedding = layers.Embedding(\n",
    "                input_dim=len(vocabulary), output_dim=embedding_dims\n",
    "            )\n",
    "\n",
    "            # Convert the index values to embedding representations.\n",
    "            encoded_categorical_feature = embedding(encoded_feature)\n",
    "            encoded_categorical_feature_list.append(encoded_categorical_feature)\n",
    "\n",
    "        else:\n",
    "\n",
    "            # Use the numerical features as-is.\n",
    "            numerical_feature = tf.expand_dims(inputs[feature_name], -1)\n",
    "            numerical_feature_list.append(numerical_feature)\n",
    "\n",
    "    return encoded_categorical_feature_list, numerical_feature_list"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0c2058ef-5e6e-47d3-b30a-bb30c1e6e3f0",
   "metadata": {},
   "source": [
    "# Implement an MLP block\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "8b00c298-edde-48ce-9c54-4a77fe897cf6",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_mlp(hidden_units, dropout_rate, activation, normalization_layer, name=None):\n",
    "\n",
    "    mlp_layers = []\n",
    "    for units in hidden_units:\n",
    "        mlp_layers.append(normalization_layer),\n",
    "        mlp_layers.append(layers.Dense(units, activation=activation))\n",
    "        mlp_layers.append(layers.Dropout(dropout_rate))\n",
    "\n",
    "    return keras.Sequential(mlp_layers, name=name)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b038800a-1d9c-4b52-9220-ba936f87a060",
   "metadata": {},
   "source": [
    "# Experiment 1: a baseline model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "84ad8659-a867-44da-a53e-8aded56b21ad",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total model weights: 109629\n",
      "You must install pydot (`pip install pydot`) and install graphviz (see instructions at https://graphviz.gitlab.io/download/) for plot_model to work.\n"
     ]
    }
   ],
   "source": [
    "#In the first experiment, we create a simple multi-layer feed-forward network\n",
    "def create_baseline_model(\n",
    "    embedding_dims, num_mlp_blocks, mlp_hidden_units_factors, dropout_rate\n",
    "):\n",
    "\n",
    "    # Create model inputs.\n",
    "    inputs = create_model_inputs()\n",
    "    # encode features.\n",
    "    encoded_categorical_feature_list, numerical_feature_list = encode_inputs(\n",
    "        inputs, embedding_dims\n",
    "    )\n",
    "    # Concatenate all features.\n",
    "    features = layers.concatenate(\n",
    "        encoded_categorical_feature_list + numerical_feature_list\n",
    "    )\n",
    "    # Compute Feedforward layer units.\n",
    "    feedforward_units = [features.shape[-1]]\n",
    "\n",
    "    # Create several feedforwad layers with skip connections.\n",
    "    for layer_idx in range(num_mlp_blocks):\n",
    "        features = create_mlp(\n",
    "            hidden_units=feedforward_units,\n",
    "            dropout_rate=dropout_rate,\n",
    "            activation=keras.activations.gelu,\n",
    "            normalization_layer=layers.LayerNormalization(epsilon=1e-6),\n",
    "            name=f\"feedforward_{layer_idx}\",\n",
    "        )(features)\n",
    "\n",
    "    # Compute MLP hidden_units.\n",
    "    mlp_hidden_units = [\n",
    "        factor * features.shape[-1] for factor in mlp_hidden_units_factors\n",
    "    ]\n",
    "    # Create final MLP.\n",
    "    features = create_mlp(\n",
    "        hidden_units=mlp_hidden_units,\n",
    "        dropout_rate=dropout_rate,\n",
    "        activation=keras.activations.selu,\n",
    "        normalization_layer=layers.BatchNormalization(),\n",
    "        name=\"MLP\",\n",
    "    )(features)\n",
    "\n",
    "    # Add a sigmoid as a binary classifer.\n",
    "    outputs = layers.Dense(units=1, activation=\"sigmoid\", name=\"sigmoid\")(features)\n",
    "    model = keras.Model(inputs=inputs, outputs=outputs)\n",
    "    return model\n",
    "\n",
    "\n",
    "baseline_model = create_baseline_model(\n",
    "    embedding_dims=EMBEDDING_DIMS,\n",
    "    num_mlp_blocks=NUM_MLP_BLOCKS,\n",
    "    mlp_hidden_units_factors=MLP_HIDDEN_UNITS_FACTORS,\n",
    "    dropout_rate=DROPOUT_RATE,\n",
    ")\n",
    "\n",
    "print(\"Total model weights:\", baseline_model.count_params())\n",
    "keras.utils.plot_model(baseline_model, show_shapes=True, rankdir=\"LR\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "7062e7fb-9e37-4904-b7e4-9cf96ac763e4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Start training the model...\n",
      "Epoch 1/15\n",
      "    122/Unknown - 4s 11ms/step - loss: 112222.4609 - accuracy: 0.7447WARNING:tensorflow:`evaluate()` received a value for `sample_weight`, but `weighted_metrics` were not provided.  Did you mean to pass metrics to `weighted_metrics` in `compile()`?  If this is intentional you can pass `weighted_metrics=[]` to `compile()` in order to silence this warning.\n",
      "123/123 [==============================] - 5s 19ms/step - loss: 112051.1484 - accuracy: 0.7449 - val_loss: 95285.3594 - val_accuracy: 0.7859\n",
      "Epoch 2/15\n",
      "120/123 [============================>.] - ETA: 0s - loss: 94127.6094 - accuracy: 0.7656WARNING:tensorflow:`evaluate()` received a value for `sample_weight`, but `weighted_metrics` were not provided.  Did you mean to pass metrics to `weighted_metrics` in `compile()`?  If this is intentional you can pass `weighted_metrics=[]` to `compile()` in order to silence this warning.\n",
      "123/123 [==============================] - 1s 11ms/step - loss: 94012.8359 - accuracy: 0.7656 - val_loss: 81845.7578 - val_accuracy: 0.7454\n",
      "Epoch 3/15\n",
      "122/123 [============================>.] - ETA: 0s - loss: 77867.2031 - accuracy: 0.7906WARNING:tensorflow:`evaluate()` received a value for `sample_weight`, but `weighted_metrics` were not provided.  Did you mean to pass metrics to `weighted_metrics` in `compile()`?  If this is intentional you can pass `weighted_metrics=[]` to `compile()` in order to silence this warning.\n",
      "123/123 [==============================] - 2s 14ms/step - loss: 77792.3281 - accuracy: 0.7905 - val_loss: 68221.2344 - val_accuracy: 0.7991\n",
      "Epoch 4/15\n",
      "119/123 [============================>.] - ETA: 0s - loss: 72870.9844 - accuracy: 0.8023WARNING:tensorflow:`evaluate()` received a value for `sample_weight`, but `weighted_metrics` were not provided.  Did you mean to pass metrics to `weighted_metrics` in `compile()`?  If this is intentional you can pass `weighted_metrics=[]` to `compile()` in order to silence this warning.\n",
      "123/123 [==============================] - 2s 13ms/step - loss: 72871.7109 - accuracy: 0.8025 - val_loss: 67260.8203 - val_accuracy: 0.8204\n",
      "Epoch 5/15\n",
      "121/123 [============================>.] - ETA: 0s - loss: 70841.4297 - accuracy: 0.8052WARNING:tensorflow:`evaluate()` received a value for `sample_weight`, but `weighted_metrics` were not provided.  Did you mean to pass metrics to `weighted_metrics` in `compile()`?  If this is intentional you can pass `weighted_metrics=[]` to `compile()` in order to silence this warning.\n",
      "123/123 [==============================] - 2s 14ms/step - loss: 70910.2500 - accuracy: 0.8052 - val_loss: 66225.4609 - val_accuracy: 0.8254\n",
      "Epoch 6/15\n",
      "120/123 [============================>.] - ETA: 0s - loss: 69813.5469 - accuracy: 0.8096WARNING:tensorflow:`evaluate()` received a value for `sample_weight`, but `weighted_metrics` were not provided.  Did you mean to pass metrics to `weighted_metrics` in `compile()`?  If this is intentional you can pass `weighted_metrics=[]` to `compile()` in order to silence this warning.\n",
      "123/123 [==============================] - 1s 12ms/step - loss: 69894.3672 - accuracy: 0.8093 - val_loss: 65757.3281 - val_accuracy: 0.8164\n",
      "Epoch 7/15\n",
      "121/123 [============================>.] - ETA: 0s - loss: 69103.5781 - accuracy: 0.8094WARNING:tensorflow:`evaluate()` received a value for `sample_weight`, but `weighted_metrics` were not provided.  Did you mean to pass metrics to `weighted_metrics` in `compile()`?  If this is intentional you can pass `weighted_metrics=[]` to `compile()` in order to silence this warning.\n",
      "123/123 [==============================] - 1s 12ms/step - loss: 69167.2734 - accuracy: 0.8093 - val_loss: 65810.7969 - val_accuracy: 0.8139\n",
      "Epoch 8/15\n",
      "121/123 [============================>.] - ETA: 0s - loss: 68022.4531 - accuracy: 0.8121WARNING:tensorflow:`evaluate()` received a value for `sample_weight`, but `weighted_metrics` were not provided.  Did you mean to pass metrics to `weighted_metrics` in `compile()`?  If this is intentional you can pass `weighted_metrics=[]` to `compile()` in order to silence this warning.\n",
      "123/123 [==============================] - 2s 12ms/step - loss: 68042.1484 - accuracy: 0.8120 - val_loss: 66298.3047 - val_accuracy: 0.8089\n",
      "Epoch 9/15\n",
      "120/123 [============================>.] - ETA: 0s - loss: 67686.7578 - accuracy: 0.8153WARNING:tensorflow:`evaluate()` received a value for `sample_weight`, but `weighted_metrics` were not provided.  Did you mean to pass metrics to `weighted_metrics` in `compile()`?  If this is intentional you can pass `weighted_metrics=[]` to `compile()` in order to silence this warning.\n",
      "123/123 [==============================] - 1s 12ms/step - loss: 67799.4688 - accuracy: 0.8151 - val_loss: 67048.1094 - val_accuracy: 0.8037\n",
      "Epoch 10/15\n",
      "119/123 [============================>.] - ETA: 0s - loss: 67180.4297 - accuracy: 0.8166WARNING:tensorflow:`evaluate()` received a value for `sample_weight`, but `weighted_metrics` were not provided.  Did you mean to pass metrics to `weighted_metrics` in `compile()`?  If this is intentional you can pass `weighted_metrics=[]` to `compile()` in order to silence this warning.\n",
      "123/123 [==============================] - 1s 12ms/step - loss: 67219.7734 - accuracy: 0.8164 - val_loss: 65703.2500 - val_accuracy: 0.8125\n",
      "Epoch 11/15\n",
      "121/123 [============================>.] - ETA: 0s - loss: 66121.9062 - accuracy: 0.8215WARNING:tensorflow:`evaluate()` received a value for `sample_weight`, but `weighted_metrics` were not provided.  Did you mean to pass metrics to `weighted_metrics` in `compile()`?  If this is intentional you can pass `weighted_metrics=[]` to `compile()` in order to silence this warning.\n",
      "123/123 [==============================] - 2s 12ms/step - loss: 66141.6094 - accuracy: 0.8215 - val_loss: 62247.5664 - val_accuracy: 0.8329\n",
      "Epoch 12/15\n",
      "121/123 [============================>.] - ETA: 0s - loss: 64629.3789 - accuracy: 0.8274WARNING:tensorflow:`evaluate()` received a value for `sample_weight`, but `weighted_metrics` were not provided.  Did you mean to pass metrics to `weighted_metrics` in `compile()`?  If this is intentional you can pass `weighted_metrics=[]` to `compile()` in order to silence this warning.\n",
      "123/123 [==============================] - 2s 13ms/step - loss: 64654.9297 - accuracy: 0.8274 - val_loss: 67286.1953 - val_accuracy: 0.8095\n",
      "Epoch 13/15\n",
      "122/123 [============================>.] - ETA: 0s - loss: 63478.0430 - accuracy: 0.8342WARNING:tensorflow:`evaluate()` received a value for `sample_weight`, but `weighted_metrics` were not provided.  Did you mean to pass metrics to `weighted_metrics` in `compile()`?  If this is intentional you can pass `weighted_metrics=[]` to `compile()` in order to silence this warning.\n",
      "123/123 [==============================] - 2s 12ms/step - loss: 63421.1602 - accuracy: 0.8344 - val_loss: 65994.8906 - val_accuracy: 0.8065\n",
      "Epoch 14/15\n",
      "120/123 [============================>.] - ETA: 0s - loss: 63270.5625 - accuracy: 0.8330WARNING:tensorflow:`evaluate()` received a value for `sample_weight`, but `weighted_metrics` were not provided.  Did you mean to pass metrics to `weighted_metrics` in `compile()`?  If this is intentional you can pass `weighted_metrics=[]` to `compile()` in order to silence this warning.\n",
      "123/123 [==============================] - 1s 12ms/step - loss: 63301.1055 - accuracy: 0.8328 - val_loss: 66033.5078 - val_accuracy: 0.8093\n",
      "Epoch 15/15\n",
      "123/123 [==============================] - ETA: 0s - loss: 62674.4102 - accuracy: 0.8331WARNING:tensorflow:`evaluate()` received a value for `sample_weight`, but `weighted_metrics` were not provided.  Did you mean to pass metrics to `weighted_metrics` in `compile()`?  If this is intentional you can pass `weighted_metrics=[]` to `compile()` in order to silence this warning.\n",
      "123/123 [==============================] - 2s 13ms/step - loss: 62674.4102 - accuracy: 0.8331 - val_loss: 61128.3711 - val_accuracy: 0.8314\n",
      "Model training finished\n",
      "WARNING:tensorflow:`evaluate()` received a value for `sample_weight`, but `weighted_metrics` were not provided.  Did you mean to pass metrics to `weighted_metrics` in `compile()`?  If this is intentional you can pass `weighted_metrics=[]` to `compile()` in order to silence this warning.\n",
      "Validation accuracy: 83.14%\n"
     ]
    }
   ],
   "source": [
    "#train and evaluate the baseline model\n",
    "history = run_experiment(\n",
    "    model=baseline_model,\n",
    "    train_data_file=train_data_file,\n",
    "    test_data_file=test_data_file,\n",
    "    num_epochs=NUM_EPOCHS,\n",
    "    learning_rate=LEARNING_RATE,\n",
    "    weight_decay=WEIGHT_DECAY,\n",
    "    batch_size=BATCH_SIZE,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "edf2e544-24df-4823-bea9-3025cf2fcd65",
   "metadata": {},
   "source": [
    "# Experiment 2: TabTransformer\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "9f103c5f-3c12-4161-818b-b16b413fd574",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total model weights: 87479\n",
      "You must install pydot (`pip install pydot`) and install graphviz (see instructions at https://graphviz.gitlab.io/download/) for plot_model to work.\n"
     ]
    }
   ],
   "source": [
    "def create_tabtransformer_classifier(\n",
    "    num_transformer_blocks,\n",
    "    num_heads,\n",
    "    embedding_dims,\n",
    "    mlp_hidden_units_factors,\n",
    "    dropout_rate,\n",
    "    use_column_embedding=False,\n",
    "):\n",
    "\n",
    "    # Create model inputs.\n",
    "    inputs = create_model_inputs()\n",
    "    # encode features.\n",
    "    encoded_categorical_feature_list, numerical_feature_list = encode_inputs(\n",
    "        inputs, embedding_dims\n",
    "    )\n",
    "    # Stack categorical feature embeddings for the Tansformer.\n",
    "    encoded_categorical_features = tf.stack(encoded_categorical_feature_list, axis=1)\n",
    "    # Concatenate numerical features.\n",
    "    numerical_features = layers.concatenate(numerical_feature_list)\n",
    "\n",
    "    # Add column embedding to categorical feature embeddings.\n",
    "    if use_column_embedding:\n",
    "        num_columns = encoded_categorical_features.shape[1]\n",
    "        column_embedding = layers.Embedding(\n",
    "            input_dim=num_columns, output_dim=embedding_dims\n",
    "        )\n",
    "        column_indices = tf.range(start=0, limit=num_columns, delta=1)\n",
    "        encoded_categorical_features = encoded_categorical_features + column_embedding(\n",
    "            column_indices\n",
    "        )\n",
    "\n",
    "    # Create multiple layers of the Transformer block.\n",
    "    for block_idx in range(num_transformer_blocks):\n",
    "        # Create a multi-head attention layer.\n",
    "        attention_output = layers.MultiHeadAttention(\n",
    "            num_heads=num_heads,\n",
    "            key_dim=embedding_dims,\n",
    "            dropout=dropout_rate,\n",
    "            name=f\"multihead_attention_{block_idx}\",\n",
    "        )(encoded_categorical_features, encoded_categorical_features)\n",
    "        # Skip connection 1.\n",
    "        x = layers.Add(name=f\"skip_connection1_{block_idx}\")(\n",
    "            [attention_output, encoded_categorical_features]\n",
    "        )\n",
    "        # Layer normalization 1.\n",
    "        x = layers.LayerNormalization(name=f\"layer_norm1_{block_idx}\", epsilon=1e-6)(x)\n",
    "        # Feedforward.\n",
    "        feedforward_output = create_mlp(\n",
    "            hidden_units=[embedding_dims],\n",
    "            dropout_rate=dropout_rate,\n",
    "            activation=keras.activations.gelu,\n",
    "            normalization_layer=layers.LayerNormalization(epsilon=1e-6),\n",
    "            name=f\"feedforward_{block_idx}\",\n",
    "        )(x)\n",
    "        # Skip connection 2.\n",
    "        x = layers.Add(name=f\"skip_connection2_{block_idx}\")([feedforward_output, x])\n",
    "        # Layer normalization 2.\n",
    "        encoded_categorical_features = layers.LayerNormalization(\n",
    "            name=f\"layer_norm2_{block_idx}\", epsilon=1e-6\n",
    "        )(x)\n",
    "\n",
    "    # Flatten the \"contextualized\" embeddings of the categorical features.\n",
    "    categorical_features = layers.Flatten()(encoded_categorical_features)\n",
    "    # Apply layer normalization to the numerical features.\n",
    "    numerical_features = layers.LayerNormalization(epsilon=1e-6)(numerical_features)\n",
    "    # Prepare the input for the final MLP block.\n",
    "    features = layers.concatenate([categorical_features, numerical_features])\n",
    "\n",
    "    # Compute MLP hidden_units.\n",
    "    mlp_hidden_units = [\n",
    "        factor * features.shape[-1] for factor in mlp_hidden_units_factors\n",
    "    ]\n",
    "    # Create final MLP.\n",
    "    features = create_mlp(\n",
    "        hidden_units=mlp_hidden_units,\n",
    "        dropout_rate=dropout_rate,\n",
    "        activation=keras.activations.selu,\n",
    "        normalization_layer=layers.BatchNormalization(),\n",
    "        name=\"MLP\",\n",
    "    )(features)\n",
    "\n",
    "    # Add a sigmoid as a binary classifer.\n",
    "    outputs = layers.Dense(units=1, activation=\"sigmoid\", name=\"sigmoid\")(features)\n",
    "    model = keras.Model(inputs=inputs, outputs=outputs)\n",
    "    return model\n",
    "\n",
    "\n",
    "tabtransformer_model = create_tabtransformer_classifier(\n",
    "    num_transformer_blocks=NUM_TRANSFORMER_BLOCKS,\n",
    "    num_heads=NUM_HEADS,\n",
    "    embedding_dims=EMBEDDING_DIMS,\n",
    "    mlp_hidden_units_factors=MLP_HIDDEN_UNITS_FACTORS,\n",
    "    dropout_rate=DROPOUT_RATE,\n",
    ")\n",
    "\n",
    "print(\"Total model weights:\", tabtransformer_model.count_params())\n",
    "keras.utils.plot_model(tabtransformer_model, show_shapes=True, rankdir=\"LR\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "876ba6fc-53ba-48af-b4d9-060f53e02e99",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Start training the model...\n",
      "Epoch 1/15\n",
      "    123/Unknown - 10s 38ms/step - loss: 81245.9922 - accuracy: 0.7964WARNING:tensorflow:`evaluate()` received a value for `sample_weight`, but `weighted_metrics` were not provided.  Did you mean to pass metrics to `weighted_metrics` in `compile()`?  If this is intentional you can pass `weighted_metrics=[]` to `compile()` in order to silence this warning.\n",
      "123/123 [==============================] - 12s 50ms/step - loss: 81245.9922 - accuracy: 0.7964 - val_loss: 63672.1172 - val_accuracy: 0.8391\n",
      "Epoch 2/15\n",
      "123/123 [==============================] - ETA: 0s - loss: 68756.9453 - accuracy: 0.8250WARNING:tensorflow:`evaluate()` received a value for `sample_weight`, but `weighted_metrics` were not provided.  Did you mean to pass metrics to `weighted_metrics` in `compile()`?  If this is intentional you can pass `weighted_metrics=[]` to `compile()` in order to silence this warning.\n",
      "123/123 [==============================] - 5s 43ms/step - loss: 68756.9453 - accuracy: 0.8250 - val_loss: 67155.2344 - val_accuracy: 0.8239\n",
      "Epoch 3/15\n",
      "123/123 [==============================] - ETA: 0s - loss: 66780.2422 - accuracy: 0.8299WARNING:tensorflow:`evaluate()` received a value for `sample_weight`, but `weighted_metrics` were not provided.  Did you mean to pass metrics to `weighted_metrics` in `compile()`?  If this is intentional you can pass `weighted_metrics=[]` to `compile()` in order to silence this warning.\n",
      "123/123 [==============================] - 5s 41ms/step - loss: 66780.2422 - accuracy: 0.8299 - val_loss: 64372.0078 - val_accuracy: 0.8310\n",
      "Epoch 4/15\n",
      "122/123 [============================>.] - ETA: 0s - loss: 64847.4258 - accuracy: 0.8359WARNING:tensorflow:`evaluate()` received a value for `sample_weight`, but `weighted_metrics` were not provided.  Did you mean to pass metrics to `weighted_metrics` in `compile()`?  If this is intentional you can pass `weighted_metrics=[]` to `compile()` in order to silence this warning.\n",
      "123/123 [==============================] - 5s 41ms/step - loss: 64816.1797 - accuracy: 0.8358 - val_loss: 64204.2227 - val_accuracy: 0.8306\n",
      "Epoch 5/15\n",
      "122/123 [============================>.] - ETA: 0s - loss: 64229.1055 - accuracy: 0.8360WARNING:tensorflow:`evaluate()` received a value for `sample_weight`, but `weighted_metrics` were not provided.  Did you mean to pass metrics to `weighted_metrics` in `compile()`?  If this is intentional you can pass `weighted_metrics=[]` to `compile()` in order to silence this warning.\n",
      "123/123 [==============================] - 5s 43ms/step - loss: 64216.4141 - accuracy: 0.8359 - val_loss: 63040.5977 - val_accuracy: 0.8350\n",
      "Epoch 6/15\n",
      "123/123 [==============================] - ETA: 0s - loss: 62998.4961 - accuracy: 0.8398WARNING:tensorflow:`evaluate()` received a value for `sample_weight`, but `weighted_metrics` were not provided.  Did you mean to pass metrics to `weighted_metrics` in `compile()`?  If this is intentional you can pass `weighted_metrics=[]` to `compile()` in order to silence this warning.\n",
      "123/123 [==============================] - 5s 40ms/step - loss: 62998.4961 - accuracy: 0.8398 - val_loss: 62053.3125 - val_accuracy: 0.8415\n",
      "Epoch 7/15\n",
      "123/123 [==============================] - ETA: 0s - loss: 62948.9453 - accuracy: 0.8392WARNING:tensorflow:`evaluate()` received a value for `sample_weight`, but `weighted_metrics` were not provided.  Did you mean to pass metrics to `weighted_metrics` in `compile()`?  If this is intentional you can pass `weighted_metrics=[]` to `compile()` in order to silence this warning.\n",
      "123/123 [==============================] - 6s 46ms/step - loss: 62948.9453 - accuracy: 0.8392 - val_loss: 61412.5391 - val_accuracy: 0.8436\n",
      "Epoch 8/15\n",
      "123/123 [==============================] - ETA: 0s - loss: 62554.7930 - accuracy: 0.8412WARNING:tensorflow:`evaluate()` received a value for `sample_weight`, but `weighted_metrics` were not provided.  Did you mean to pass metrics to `weighted_metrics` in `compile()`?  If this is intentional you can pass `weighted_metrics=[]` to `compile()` in order to silence this warning.\n",
      "123/123 [==============================] - 5s 43ms/step - loss: 62554.7930 - accuracy: 0.8412 - val_loss: 60943.1641 - val_accuracy: 0.8472\n",
      "Epoch 9/15\n",
      "123/123 [==============================] - ETA: 0s - loss: 62237.2812 - accuracy: 0.8411WARNING:tensorflow:`evaluate()` received a value for `sample_weight`, but `weighted_metrics` were not provided.  Did you mean to pass metrics to `weighted_metrics` in `compile()`?  If this is intentional you can pass `weighted_metrics=[]` to `compile()` in order to silence this warning.\n",
      "123/123 [==============================] - 5s 44ms/step - loss: 62237.2812 - accuracy: 0.8411 - val_loss: 60881.2852 - val_accuracy: 0.8460\n",
      "Epoch 10/15\n",
      "122/123 [============================>.] - ETA: 0s - loss: 61846.9375 - accuracy: 0.8405WARNING:tensorflow:`evaluate()` received a value for `sample_weight`, but `weighted_metrics` were not provided.  Did you mean to pass metrics to `weighted_metrics` in `compile()`?  If this is intentional you can pass `weighted_metrics=[]` to `compile()` in order to silence this warning.\n",
      "123/123 [==============================] - 6s 50ms/step - loss: 61858.7930 - accuracy: 0.8404 - val_loss: 60918.2070 - val_accuracy: 0.8464\n",
      "Epoch 11/15\n",
      "122/123 [============================>.] - ETA: 0s - loss: 61877.0586 - accuracy: 0.8428WARNING:tensorflow:`evaluate()` received a value for `sample_weight`, but `weighted_metrics` were not provided.  Did you mean to pass metrics to `weighted_metrics` in `compile()`?  If this is intentional you can pass `weighted_metrics=[]` to `compile()` in order to silence this warning.\n",
      "123/123 [==============================] - 6s 47ms/step - loss: 61904.1250 - accuracy: 0.8427 - val_loss: 60904.8164 - val_accuracy: 0.8452\n",
      "Epoch 12/15\n",
      "122/123 [============================>.] - ETA: 0s - loss: 61444.7891 - accuracy: 0.8429WARNING:tensorflow:`evaluate()` received a value for `sample_weight`, but `weighted_metrics` were not provided.  Did you mean to pass metrics to `weighted_metrics` in `compile()`?  If this is intentional you can pass `weighted_metrics=[]` to `compile()` in order to silence this warning.\n",
      "123/123 [==============================] - 7s 54ms/step - loss: 61426.9336 - accuracy: 0.8428 - val_loss: 60854.5078 - val_accuracy: 0.8457\n",
      "Epoch 13/15\n",
      "122/123 [============================>.] - ETA: 0s - loss: 61452.6328 - accuracy: 0.8423WARNING:tensorflow:`evaluate()` received a value for `sample_weight`, but `weighted_metrics` were not provided.  Did you mean to pass metrics to `weighted_metrics` in `compile()`?  If this is intentional you can pass `weighted_metrics=[]` to `compile()` in order to silence this warning.\n",
      "123/123 [==============================] - 6s 51ms/step - loss: 61458.4023 - accuracy: 0.8422 - val_loss: 60702.8398 - val_accuracy: 0.8454\n",
      "Epoch 14/15\n",
      "123/123 [==============================] - ETA: 0s - loss: 61192.3867 - accuracy: 0.8454WARNING:tensorflow:`evaluate()` received a value for `sample_weight`, but `weighted_metrics` were not provided.  Did you mean to pass metrics to `weighted_metrics` in `compile()`?  If this is intentional you can pass `weighted_metrics=[]` to `compile()` in order to silence this warning.\n",
      "123/123 [==============================] - 6s 48ms/step - loss: 61192.3867 - accuracy: 0.8454 - val_loss: 60703.4023 - val_accuracy: 0.8455\n",
      "Epoch 15/15\n",
      "122/123 [============================>.] - ETA: 0s - loss: 61068.8633 - accuracy: 0.8452WARNING:tensorflow:`evaluate()` received a value for `sample_weight`, but `weighted_metrics` were not provided.  Did you mean to pass metrics to `weighted_metrics` in `compile()`?  If this is intentional you can pass `weighted_metrics=[]` to `compile()` in order to silence this warning.\n",
      "123/123 [==============================] - 5s 43ms/step - loss: 61096.4336 - accuracy: 0.8450 - val_loss: 60942.1367 - val_accuracy: 0.8453\n",
      "Model training finished\n",
      "WARNING:tensorflow:`evaluate()` received a value for `sample_weight`, but `weighted_metrics` were not provided.  Did you mean to pass metrics to `weighted_metrics` in `compile()`?  If this is intentional you can pass `weighted_metrics=[]` to `compile()` in order to silence this warning.\n",
      "Validation accuracy: 84.53%\n"
     ]
    }
   ],
   "source": [
    "#train and evaluate the TabTransformer model\n",
    "history = run_experiment(\n",
    "    model=tabtransformer_model,\n",
    "    train_data_file=train_data_file,\n",
    "    test_data_file=test_data_file,\n",
    "    num_epochs=NUM_EPOCHS,\n",
    "    learning_rate=LEARNING_RATE,\n",
    "    weight_decay=WEIGHT_DECAY,\n",
    "    batch_size=BATCH_SIZE,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "620f3df6-84f8-49ea-98df-9a065264b311",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
